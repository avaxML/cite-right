{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cite-Right guide\n",
    "\n",
    "This notebook demonstrates all features, options, and extras of the cite-right library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation Options\n",
    "\n",
    "Cite-Right has several optional extras you can install:\n",
    "\n",
    "```bash\n",
    "# Basic installation\n",
    "pip install cite-right\n",
    "\n",
    "# With embeddings (sentence-transformers)\n",
    "pip install \"cite-right[embeddings]\"\n",
    "\n",
    "# With tiktoken tokenizer (OpenAI's tokenizer)\n",
    "pip install \"cite-right[tiktoken]\"\n",
    "\n",
    "# With HuggingFace tokenizers\n",
    "pip install \"cite-right[huggingface]\"\n",
    "\n",
    "# With spaCy for better sentence segmentation and claim decomposition\n",
    "pip install \"cite-right[spacy]\"\n",
    "python -m spacy download en_core_web_sm\n",
    "\n",
    "# With pySBD for fast sentence boundary detection\n",
    "pip install \"cite-right[pysbd]\"\n",
    "\n",
    "# With LangChain integration\n",
    "pip install \"cite-right[langchain]\"\n",
    "\n",
    "# With LlamaIndex integration\n",
    "pip install \"cite-right[llamaindex]\"\n",
    "\n",
    "# Install everything!\n",
    "pip install \"cite-right[embeddings,tiktoken,huggingface,spacy,pysbd,langchain,llamaindex]\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Core Citation Alignment\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Basic Usage with `align_citations()`\n",
    "\n",
    "The core function that aligns answer spans to source documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer span: 'Global temperatures have increased by 1.1°C since the pre-industrial era.'\n",
      "Status: supported\n",
      "Char range: [0:73]\n",
      "  Source: climate_report (index 0)\n",
      "  Evidence: 'Global temperatures have risen by 1.1°C since pre-industrial'\n",
      "  Evidence offsets: [0:60]\n",
      "  Score: 1.760\n",
      "  Answer coverage: 72.7%\n",
      "\n",
      "Answer span: 'Renewable energy sources like solar and wind now generate 12% of global electricity.'\n",
      "Status: supported\n",
      "Char range: [74:158]\n",
      "  Source: renewable_energy (index 1)\n",
      "  Evidence: 'Solar and wind power now account for 12% of global electricity'\n",
      "  Evidence offsets: [0:62]\n",
      "  Score: 1.532\n",
      "  Answer coverage: 64.3%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from cite_right import SourceDocument, align_citations\n",
    "\n",
    "# Define source documents\n",
    "sources = [\n",
    "    SourceDocument(\n",
    "        id=\"climate_report\",\n",
    "        text=\"Global temperatures have risen by 1.1°C since pre-industrial times. \"\n",
    "        \"The rate of warming has accelerated in recent decades.\",\n",
    "        metadata={\"year\": 2024, \"type\": \"report\"},  # Optional metadata\n",
    "    ),\n",
    "    SourceDocument(\n",
    "        id=\"renewable_energy\",\n",
    "        text=\"Solar and wind power now account for 12% of global electricity generation. \"\n",
    "        \"Investment in renewable energy reached $500 billion in 2023.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# An AI-generated answer\n",
    "answer = (\n",
    "    \"Global temperatures have increased by 1.1°C since the pre-industrial era. \"\n",
    "    \"Renewable energy sources like solar and wind now generate 12% of global electricity.\"\n",
    ")\n",
    "\n",
    "# Align citations\n",
    "results = align_citations(answer, sources)\n",
    "\n",
    "# Display results\n",
    "for span_citations in results:\n",
    "    print(f\"Answer span: {span_citations.answer_span.text!r}\")\n",
    "    print(f\"Status: {span_citations.status}\")\n",
    "    print(\n",
    "        f\"Char range: [{span_citations.answer_span.char_start}:{span_citations.answer_span.char_end}]\"\n",
    "    )\n",
    "    for citation in span_citations.citations:\n",
    "        print(f\"  Source: {citation.source_id} (index {citation.source_index})\")\n",
    "        print(f\"  Evidence: {citation.evidence!r}\")\n",
    "        print(f\"  Evidence offsets: [{citation.char_start}:{citation.char_end}]\")\n",
    "        print(f\"  Score: {citation.score:.3f}\")\n",
    "        print(f\"  Answer coverage: {citation.components.get('answer_coverage', 0):.1%}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Using Plain Strings as Sources\n",
    "\n",
    "You can also pass plain strings instead of `SourceDocument` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: partial\n",
      "Source ID: 0\n"
     ]
    }
   ],
   "source": [
    "from cite_right import align_citations\n",
    "\n",
    "# Sources as plain strings (IDs will be \"0\", \"1\", etc.)\n",
    "sources = [\n",
    "    \"Revenue grew 15% in Q4 2024, driven by strong holiday sales.\",\n",
    "    \"The company announced plans to expand into 5 new markets next year.\",\n",
    "]\n",
    "\n",
    "answer = \"Revenue increased by 15% in the fourth quarter.\"\n",
    "\n",
    "results = align_citations(answer, sources)\n",
    "for sc in results:\n",
    "    print(f\"Status: {sc.status}\")\n",
    "    if sc.citations:\n",
    "        print(f\"Source ID: {sc.citations[0].source_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Using `SourceChunk` for Pre-chunked Documents\n",
    "\n",
    "Use `SourceChunk` when you have pre-chunked documents from a RAG pipeline and want citation offsets relative to the original document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'Revenue was $50M, a 15% increase year-over-year.'\n",
      "  Evidence: 'venue reached $50M, up 15% Y'\n",
      "  Offsets in original doc: [71:99]\n",
      "  Verification: 'venue reached $50M, up 15% Y'\n"
     ]
    }
   ],
   "source": [
    "from cite_right import SourceChunk, align_citations\n",
    "\n",
    "# Original full document\n",
    "full_document = (\n",
    "    \"Introduction: This report covers Q4 2024 performance. \"\n",
    "    \"Revenue: Total revenue reached $50M, up 15% YoY. \"\n",
    "    \"Outlook: We expect continued growth in 2025.\"\n",
    ")\n",
    "\n",
    "# Pre-chunked sections (e.g., from a RAG retrieval)\n",
    "chunks = [\n",
    "    SourceChunk(\n",
    "        source_id=\"report\",\n",
    "        text=\"Revenue: Total revenue reached $50M, up 15% YoY.\",\n",
    "        doc_char_start=56,  # Where this chunk starts in full document\n",
    "        doc_char_end=104,  # Where this chunk ends\n",
    "        document_text=full_document,  # Optional: enables absolute offset computation\n",
    "        source_index=0,  # Optional: custom source index\n",
    "    ),\n",
    "    SourceChunk(\n",
    "        source_id=\"report\",\n",
    "        text=\"Outlook: We expect continued growth in 2025.\",\n",
    "        doc_char_start=105,\n",
    "        doc_char_end=149,\n",
    "        document_text=full_document,\n",
    "    ),\n",
    "]\n",
    "\n",
    "answer = \"Revenue was $50M, a 15% increase year-over-year.\"\n",
    "\n",
    "results = align_citations(answer, chunks)\n",
    "\n",
    "for sc in results:\n",
    "    print(f\"Answer: {sc.answer_span.text!r}\")\n",
    "    for citation in sc.citations:\n",
    "        print(f\"  Evidence: {citation.evidence!r}\")\n",
    "        print(f\"  Offsets in original doc: [{citation.char_start}:{citation.char_end}]\")\n",
    "        # Verify the offset works\n",
    "        print(\n",
    "            f\"  Verification: {full_document[citation.char_start : citation.char_end]!r}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Configuration Options\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Configuration Presets\n",
    "\n",
    "`CitationConfig` provides presets for common use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced: top_k=3, min_answer_coverage=0.2\n",
      "Strict: top_k=2, min_answer_coverage=0.4, supported_threshold=0.7\n",
      "Permissive: top_k=5, allow_embedding_only=True\n",
      "Fast: top_k=1, max_candidates_total=100\n",
      "\n",
      "--- Results with different presets ---\n",
      "balanced    : status=supported, citations=1\n",
      "strict      : status=partial, citations=1\n",
      "permissive  : status=supported, citations=1\n",
      "fast        : status=supported, citations=1\n"
     ]
    }
   ],
   "source": [
    "from cite_right import CitationConfig, SourceDocument, align_citations\n",
    "\n",
    "sources = [\n",
    "    SourceDocument(\n",
    "        id=\"doc\",\n",
    "        text=\"The product launch was successful with 10,000 units sold in the first week.\",\n",
    "    )\n",
    "]\n",
    "answer = \"The launch was a success.\"\n",
    "\n",
    "# Balanced (default) - good for general use\n",
    "balanced = CitationConfig.balanced()\n",
    "print(\n",
    "    f\"Balanced: top_k={balanced.top_k}, min_answer_coverage={balanced.min_answer_coverage}\"\n",
    ")\n",
    "\n",
    "# Strict - high precision, minimize false positives\n",
    "strict = CitationConfig.strict()\n",
    "print(\n",
    "    f\"Strict: top_k={strict.top_k}, min_answer_coverage={strict.min_answer_coverage}, supported_threshold={strict.supported_answer_coverage}\"\n",
    ")\n",
    "\n",
    "# Permissive - lenient, good for paraphrased content\n",
    "permissive = CitationConfig.permissive()\n",
    "print(\n",
    "    f\"Permissive: top_k={permissive.top_k}, allow_embedding_only={permissive.allow_embedding_only}\"\n",
    ")\n",
    "\n",
    "# Fast - optimized for speed\n",
    "fast = CitationConfig.fast()\n",
    "print(f\"Fast: top_k={fast.top_k}, max_candidates_total={fast.max_candidates_total}\")\n",
    "\n",
    "# Compare results\n",
    "print(\"\\n--- Results with different presets ---\")\n",
    "for name, config in [\n",
    "    (\"balanced\", balanced),\n",
    "    (\"strict\", strict),\n",
    "    (\"permissive\", permissive),\n",
    "    (\"fast\", fast),\n",
    "]:\n",
    "    results = align_citations(answer, sources, config=config)\n",
    "    status = results[0].status if results else \"no results\"\n",
    "    num_citations = len(results[0].citations) if results else 0\n",
    "    print(f\"{name:12s}: status={status}, citations={num_citations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Key CitationConfig Options\n",
    "\n",
    "The most commonly customized options (see docs for the full list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_k=3, supported_threshold=0.6\n"
     ]
    }
   ],
   "source": [
    "from cite_right import CitationConfig, CitationWeights\n",
    "\n",
    "config = CitationConfig(\n",
    "    # Result limits\n",
    "    top_k=3,  # Max citations per answer span\n",
    "    max_citations_per_source=2,  # Max citations from same source\n",
    "    # Quality thresholds\n",
    "    min_answer_coverage=0.2,  # Min fraction of answer tokens matched\n",
    "    supported_answer_coverage=0.6,  # Threshold for \"supported\" status\n",
    "    # Embedding options (when using an embedder)\n",
    "    allow_embedding_only=False,  # Allow citations with only semantic match\n",
    "    # Multi-span evidence\n",
    "    multi_span_evidence=False,  # Enable non-contiguous evidence spans\n",
    "    # Score weights\n",
    "    weights=CitationWeights(\n",
    "        alignment=1.0,\n",
    "        answer_coverage=1.0,\n",
    "        lexical=0.5,\n",
    "        embedding=0.5,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"top_k={config.top_k}, supported_threshold={config.supported_answer_coverage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Multi-Span Evidence\n",
    "\n",
    "When enabled, citations can include multiple non-contiguous evidence spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'Revenue grew 15% and profits increased by 20%.'\n",
      "  Number of evidence spans: 1\n",
      "    Span 1: [65:94] 'Profits also increased by 20%'\n",
      "  Number of evidence spans: 1\n",
      "    Span 1: [0:16] 'Revenue grew 15%'\n"
     ]
    }
   ],
   "source": [
    "from cite_right import CitationConfig, SourceDocument, align_citations\n",
    "\n",
    "# Source with relevant info spread across the text\n",
    "sources = [\n",
    "    SourceDocument(\n",
    "        id=\"report\",\n",
    "        text=\"Revenue grew 15% this year. The company hired 500 new employees. Profits also increased by 20%.\",\n",
    "    )\n",
    "]\n",
    "\n",
    "answer = \"Revenue grew 15% and profits increased by 20%.\"\n",
    "\n",
    "# Enable multi-span evidence\n",
    "config = CitationConfig(\n",
    "    multi_span_evidence=True,\n",
    "    multi_span_merge_gap_chars=16,  # Merge spans within 16 chars\n",
    "    multi_span_max_spans=5,  # Allow up to 5 spans\n",
    ")\n",
    "\n",
    "results = align_citations(answer, sources, config=config)\n",
    "\n",
    "for sc in results:\n",
    "    print(f\"Answer: {sc.answer_span.text!r}\")\n",
    "    for citation in sc.citations:\n",
    "        print(f\"  Number of evidence spans: {len(citation.evidence_spans)}\")\n",
    "        for i, span in enumerate(citation.evidence_spans):\n",
    "            print(\n",
    "                f\"    Span {i + 1}: [{span.char_start}:{span.char_end}] {span.evidence!r}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Backend Selection: Python vs Rust\n",
    "\n",
    "Choose between pure Python or the faster Rust backend for alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto backend: 3.02ms\n",
      "Python backend: 4.59ms\n",
      "Rust backend: 2.76ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from cite_right import SourceDocument, align_citations\n",
    "\n",
    "sources = [\n",
    "    SourceDocument(id=\"doc\", text=\"The quick brown fox jumps over the lazy dog. \" * 100)\n",
    "]\n",
    "answer = \"The brown fox jumped over a lazy dog.\"\n",
    "\n",
    "# Auto (default) - uses Rust if available, falls back to Python\n",
    "start = time.perf_counter()\n",
    "results_auto = align_citations(answer, sources, backend=\"auto\")\n",
    "print(f\"Auto backend: {(time.perf_counter() - start) * 1000:.2f}ms\")\n",
    "\n",
    "# Force Python backend\n",
    "start = time.perf_counter()\n",
    "results_python = align_citations(answer, sources, backend=\"python\")\n",
    "print(f\"Python backend: {(time.perf_counter() - start) * 1000:.2f}ms\")\n",
    "\n",
    "# Force Rust backend (will error if not installed)\n",
    "try:\n",
    "    start = time.perf_counter()\n",
    "    results_rust = align_citations(answer, sources, backend=\"rust\")\n",
    "    print(f\"Rust backend: {(time.perf_counter() - start) * 1000:.2f}ms\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"Rust backend not available: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Metrics Callback for Observability\n",
    "\n",
    "Use `on_metrics` to receive detailed timing and statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Alignment Metrics ===\n",
      "Total time: 0.14ms\n",
      "Number of answer spans: 2\n",
      "Number of candidates: 2\n",
      "Number of alignments: 2\n",
      "Embedding time: 0.00ms\n",
      "Alignment time: 0.04ms\n"
     ]
    }
   ],
   "source": [
    "from cite_right import AlignmentMetrics, SourceDocument, align_citations\n",
    "\n",
    "sources = [\n",
    "    SourceDocument(id=\"doc1\", text=\"Revenue grew 15% in Q4 2024.\"),\n",
    "    SourceDocument(id=\"doc2\", text=\"The company expanded into new markets.\"),\n",
    "]\n",
    "answer = \"Revenue increased by 15%. The company expanded globally.\"\n",
    "\n",
    "\n",
    "# Define a callback to receive metrics\n",
    "def metrics_callback(metrics: AlignmentMetrics) -> None:\n",
    "    print(\"=== Alignment Metrics ===\")\n",
    "    print(f\"Total time: {metrics.total_time_ms:.2f}ms\")\n",
    "    print(f\"Number of answer spans: {metrics.num_answer_spans}\")\n",
    "    print(f\"Number of candidates: {metrics.num_candidates}\")\n",
    "    print(f\"Number of alignments: {metrics.num_alignments}\")\n",
    "    print(f\"Embedding time: {metrics.embedding_time_ms:.2f}ms\")\n",
    "    print(f\"Alignment time: {metrics.alignment_time_ms:.2f}ms\")\n",
    "\n",
    "\n",
    "results = align_citations(answer, sources, on_metrics=metrics_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Tokenizers\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 SimpleTokenizer (Default)\n",
    "\n",
    "Rule-based tokenizer with number/currency/percent normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Revenue grew 15% to $50M.\n",
      "Token IDs: [1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Token spans: [(0, 7), (8, 12), (13, 15), (15, 16), (17, 19), (20, 21), (21, 23), (23, 24)]\n",
      "\n",
      "With normalization disabled:\n",
      "Token IDs: [1, 2, 3, 4, 5, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "from cite_right import SimpleTokenizer, TokenizerConfig\n",
    "\n",
    "# Default configuration\n",
    "tokenizer = SimpleTokenizer()\n",
    "result = tokenizer.tokenize(\"Revenue grew 15% to $50M.\")\n",
    "print(f\"Text: {result.text}\")\n",
    "print(f\"Token IDs: {result.token_ids}\")\n",
    "print(f\"Token spans: {result.token_spans}\")\n",
    "\n",
    "# Custom configuration - disable normalizations\n",
    "config = TokenizerConfig(\n",
    "    normalize_numbers=False,  # Keep \"1,000\" as-is instead of \"1000\"\n",
    "    normalize_percent=False,  # Keep \"%\" instead of \"percent\"\n",
    "    normalize_currency=False,  # Keep \"$\" instead of \"dollar\"\n",
    ")\n",
    "tokenizer_raw = SimpleTokenizer(config)\n",
    "result_raw = tokenizer_raw.tokenize(\"Revenue grew 15% to $50M.\")\n",
    "print(\"\\nWith normalization disabled:\")\n",
    "print(f\"Token IDs: {result_raw.token_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 TiktokenTokenizer (OpenAI)\n",
    "\n",
    "Uses OpenAI's tiktoken for tokenization. Install with `pip install cite-right[tiktoken]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding: cl100k_base\n",
      "Token IDs: [9906, 11, 1917, 0, 1115, 374, 264, 1296, 13]\n",
      "Token spans: [(0, 5), (5, 6), (6, 12), (12, 13), (13, 18), (18, 21), (21, 23), (23, 28), (28, 29)]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from cite_right.text.tokenizer_tiktoken import TiktokenTokenizer\n",
    "\n",
    "    tokenizer = TiktokenTokenizer(\"cl100k_base\")\n",
    "    result = tokenizer.tokenize(\"Hello, world! This is a test.\")\n",
    "    print(\"Encoding: cl100k_base\")\n",
    "    print(f\"Token IDs: {result.token_ids}\")\n",
    "    print(f\"Token spans: {result.token_spans}\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\n",
    "        \"TiktokenTokenizer not available. Install with: pip install cite-right[tiktoken]\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 HuggingFaceTokenizer\n",
    "\n",
    "Use any HuggingFace tokenizer. Install with `pip install cite-right[huggingface]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/cite-env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen3-235B-A22B-Thinking-2507-FP8\n",
      "Token IDs: [9707, 11, 1879, 0, 1096, 374, 264, 1273, 13]\n",
      "Token spans: [(0, 5), (5, 6), (6, 12), (12, 13), (13, 18), (18, 21), (21, 23), (23, 28), (28, 29)]\n",
      "\n",
      "Alignment status: supported\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from cite_right import SourceDocument, align_citations\n",
    "    from cite_right.text.tokenizer_huggingface import HuggingFaceTokenizer\n",
    "\n",
    "    # Load tokenizer from HuggingFace Hub using from_pretrained()\n",
    "    # Note: Don't pass a string to the constructor - use from_pretrained()!\n",
    "    tokenizer = HuggingFaceTokenizer.from_pretrained(\n",
    "        \"Qwen/Qwen3-235B-A22B-Thinking-2507-FP8\"\n",
    "    )\n",
    "    result = tokenizer.tokenize(\"Hello, world! This is a test.\")\n",
    "    print(\"Model: Qwen/Qwen3-235B-A22B-Thinking-2507-FP8\")\n",
    "    print(f\"Token IDs: {result.token_ids}\")\n",
    "    print(f\"Token spans: {result.token_spans}\")\n",
    "\n",
    "    # Works with any HuggingFace model\n",
    "    # tokenizer = HuggingFaceTokenizer.from_pretrained(\"gpt2\")\n",
    "    # tokenizer = HuggingFaceTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "    # Use with align_citations\n",
    "    sources = [SourceDocument(id=\"doc\", text=\"The quick brown fox jumps.\")]\n",
    "    answer = \"The brown fox jumped.\"\n",
    "    results = align_citations(answer, sources, tokenizer=tokenizer)\n",
    "    print(f\"\\nAlignment status: {results[0].status}\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\n",
    "        \"HuggingFaceTokenizer not available. Install with: pip install cite-right[huggingface]\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Segmenters\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 SimpleSegmenter (Default)\n",
    "\n",
    "Rule-based sentence segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0:3] 'Dr.'\n",
      "[4:28] 'Smith went to the store.'\n",
      "[29:44] 'He bought milk.'\n",
      "[45:65] 'The price was $3.50.'\n"
     ]
    }
   ],
   "source": [
    "from cite_right.text.segmenter_simple import SimpleSegmenter\n",
    "\n",
    "segmenter = SimpleSegmenter()\n",
    "text = \"Dr. Smith went to the store. He bought milk. The price was $3.50.\"\n",
    "\n",
    "segments = segmenter.segment(text)\n",
    "for seg in segments:\n",
    "    print(f\"[{seg.doc_char_start}:{seg.doc_char_end}] {seg.text!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 PySBDSegmenter\n",
    "\n",
    "Fast, rule-based sentence boundary detection. Install with `pip install cite-right[pysbd]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySBD Segmentation:\n",
      "  [0:28] 'Dr. Smith went to the store.'\n",
      "  [29:44] 'He bought milk.'\n",
      "  [45:65] 'The price was $3.50.'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from cite_right import PySBDSegmenter\n",
    "\n",
    "    # English (default)\n",
    "    segmenter = PySBDSegmenter(language=\"en\")\n",
    "    text = \"Dr. Smith went to the store. He bought milk. The price was $3.50.\"\n",
    "\n",
    "    segments = segmenter.segment(text)\n",
    "    print(\"PySBD Segmentation:\")\n",
    "    for seg in segments:\n",
    "        print(f\"  [{seg.doc_char_start}:{seg.doc_char_end}] {seg.text!r}\")\n",
    "\n",
    "    # Multi-language support\n",
    "    # german_segmenter = PySBDSegmenter(language=\"de\")\n",
    "    # spanish_segmenter = PySBDSegmenter(language=\"es\")\n",
    "\n",
    "except RuntimeError:\n",
    "    print(\"PySBDSegmenter not available. Install with: pip install cite-right[pysbd]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 SpacySegmenter\n",
    "\n",
    "spaCy-based segmentation with clause splitting. Install with `pip install cite-right[spacy]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySBD (sentences only - no clause splitting):\n",
      "  [0:28] 'Dr. Smith went to the store.'\n",
      "  [29:63] 'He bought milk, and he paid $3.50.'\n",
      "\n",
      "spaCy (with clause splitting at conjunctions):\n",
      "  [0:28] 'Dr. Smith went to the store.'\n",
      "  [29:44] 'He bought milk,'\n",
      "  [49:63] 'he paid $3.50.'\n",
      "\n",
      "→ Notice SpacySegmenter splits 'He bought milk, and he paid $3.50'\n",
      "  into two clauses at the 'and' conjunction!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from cite_right import PySBDSegmenter, SpacySegmenter\n",
    "\n",
    "    # Text with a conjunction that can be split at clause level\n",
    "    text = \"Dr. Smith went to the store. He bought milk, and he paid $3.50.\"\n",
    "\n",
    "    # PySBD: Sentence-level only (no clause splitting)\n",
    "    pysbd = PySBDSegmenter()\n",
    "    segments = pysbd.segment(text)\n",
    "    print(\"PySBD (sentences only - no clause splitting):\")\n",
    "    for seg in segments:\n",
    "        print(f\"  [{seg.doc_char_start}:{seg.doc_char_end}] {seg.text!r}\")\n",
    "\n",
    "    # SpacySegmenter: Splits at clause conjunctions (\"and\", \"or\", \"but\")\n",
    "    spacy_seg = SpacySegmenter(model=\"en_core_web_sm\")\n",
    "    segments = spacy_seg.segment(text)\n",
    "    print(\"\\nspaCy (with clause splitting at conjunctions):\")\n",
    "    for seg in segments:\n",
    "        print(f\"  [{seg.doc_char_start}:{seg.doc_char_end}] {seg.text!r}\")\n",
    "\n",
    "    print(\"\\n→ Notice SpacySegmenter splits 'He bought milk, and he paid $3.50'\")\n",
    "    print(\"  into two clauses at the 'and' conjunction!\")\n",
    "\n",
    "except RuntimeError as e:\n",
    "    print(f\"Segmenter not available: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Answer Segmenters\n",
    "\n",
    "Control how the answer text is split into spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleAnswerSegmenter (sentence-level):\n",
      "  [0] 'Revenue grew 15% in Q4, and profits also increased.'\n",
      "  [1] 'The company expanded into new markets, but they faced challenges.'\n",
      "\n",
      "SpacyAnswerSegmenter (with clause splitting):\n",
      "  [0] 'Revenue grew 15% in Q4,'\n",
      "  [1] 'profits also increased.'\n",
      "  [2] 'The company expanded into new markets,'\n",
      "  [3] 'they faced challenges.'\n",
      "\n",
      "→ Notice SpacyAnswerSegmenter splits at 'and' and 'but' conjunctions!\n"
     ]
    }
   ],
   "source": [
    "# Text with conjunctions to demonstrate clause splitting\n",
    "# SimpleAnswerSegmenter: sentence-level only\n",
    "from cite_right.text.answer_segmenter import SimpleAnswerSegmenter\n",
    "\n",
    "answer = \"\"\"Revenue grew 15% in Q4, and profits also increased.\n",
    "\n",
    "The company expanded into new markets, but they faced challenges.\"\"\"\n",
    "\n",
    "simple = SimpleAnswerSegmenter()\n",
    "spans = simple.segment(answer)\n",
    "print(\"SimpleAnswerSegmenter (sentence-level):\")\n",
    "for span in spans:\n",
    "    print(f\"  [{span.sentence_index}] {span.text!r}\")\n",
    "\n",
    "# SpacyAnswerSegmenter with clause splitting: breaks at \"and\", \"but\"\n",
    "try:\n",
    "    from cite_right import SpacyAnswerSegmenter\n",
    "\n",
    "    spacy_clause = SpacyAnswerSegmenter(model=\"en_core_web_sm\", split_clauses=True)\n",
    "    spans = spacy_clause.segment(answer)\n",
    "    print(\"\\nSpacyAnswerSegmenter (with clause splitting):\")\n",
    "    for span in spans:\n",
    "        print(f\"  [{span.sentence_index}] {span.text!r}\")\n",
    "\n",
    "    print(\"\\n→ Notice SpacyAnswerSegmenter splits at 'and' and 'but' conjunctions!\")\n",
    "\n",
    "except RuntimeError:\n",
    "    print(\"\\nSpacyAnswerSegmenter not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Embeddings\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 SentenceTransformerEmbedder\n",
    "\n",
    "Embeddings help find citations when the answer paraphrases the source. Install with `pip install cite-right[embeddings]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without embeddings: status=unsupported\n",
      "  Citations found: 0\n",
      "\n",
      "With embeddings: status=partial\n",
      "  Embedding score: 0.591\n",
      "  Evidence: 'Our approach requires very few computing cycles and has low memory overhead.'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from cite_right import (\n",
    "        CitationConfig,\n",
    "        SentenceTransformerEmbedder,\n",
    "        SourceDocument,\n",
    "        align_citations,\n",
    "    )\n",
    "\n",
    "    embedder = SentenceTransformerEmbedder(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    # Semantically similar but lexically different (paraphrased)\n",
    "    answer = \"The method is computationally efficient and uses minimal resources.\"\n",
    "    sources = [\n",
    "        SourceDocument(\n",
    "            id=\"paper\",\n",
    "            text=\"Our approach requires very few computing cycles and has low memory overhead.\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Without embeddings - low lexical overlap means no match\n",
    "    results_no_embed = align_citations(answer, sources)\n",
    "    print(f\"Without embeddings: status={results_no_embed[0].status}\")\n",
    "    print(f\"  Citations found: {len(results_no_embed[0].citations)}\")\n",
    "\n",
    "    # With embeddings + allow_embedding_only - semantic similarity finds the match\n",
    "    config = CitationConfig(allow_embedding_only=True, min_embedding_similarity=0.3)\n",
    "    results_with_embed = align_citations(\n",
    "        answer, sources, embedder=embedder, config=config\n",
    "    )\n",
    "    print(f\"\\nWith embeddings: status={results_with_embed[0].status}\")\n",
    "    if results_with_embed[0].citations:\n",
    "        c = results_with_embed[0].citations[0]\n",
    "        print(f\"  Embedding score: {c.components.get('embedding_score', 0):.3f}\")\n",
    "        print(f\"  Evidence: {c.evidence!r}\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Embeddings not available. Install with: pip install cite-right[embeddings]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Convenience Functions\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Quick Groundedness Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Global temperatures have risen by 1.1°C.'\n",
      "  is_grounded(threshold=0.5): True\n",
      "  is_hallucinated(threshold=0.3): False\n",
      "\n",
      "'The ice caps will completely melt by 2030.'\n",
      "  is_grounded(threshold=0.5): False\n",
      "  is_hallucinated(threshold=0.3): True\n"
     ]
    }
   ],
   "source": [
    "from cite_right import is_grounded, is_hallucinated\n",
    "\n",
    "sources = [\"Global temperatures have risen by 1.1°C since pre-industrial times.\"]\n",
    "\n",
    "# Well-grounded answer\n",
    "grounded = \"Global temperatures have risen by 1.1°C.\"\n",
    "print(f\"'{grounded}'\")\n",
    "print(f\"  is_grounded(threshold=0.5): {is_grounded(grounded, sources, threshold=0.5)}\")\n",
    "print(\n",
    "    f\"  is_hallucinated(threshold=0.3): {is_hallucinated(grounded, sources, threshold=0.3)}\"\n",
    ")\n",
    "\n",
    "# Potentially hallucinated answer\n",
    "hallucinated = \"The ice caps will completely melt by 2030.\"\n",
    "print(f\"\\n'{hallucinated}'\")\n",
    "print(\n",
    "    f\"  is_grounded(threshold=0.5): {is_grounded(hallucinated, sources, threshold=0.5)}\"\n",
    ")\n",
    "print(\n",
    "    f\"  is_hallucinated(threshold=0.3): {is_hallucinated(hallucinated, sources, threshold=0.3)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Detailed Groundedness Metrics with `check_groundedness()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Groundedness Metrics ===\n",
      "Groundedness Score: 59.4%\n",
      "Hallucination Rate: 40.6%\n",
      "Average Confidence: 58.3%\n",
      "Minimum Confidence: 0.0%\n",
      "\n",
      "Span breakdown:\n",
      "  Total spans: 4\n",
      "  Supported: 2 (52.4%)\n",
      "  Partial: 1 (21.0%)\n",
      "  Unsupported: 1 (26.6%)\n",
      "  Weak citations: 1\n",
      "\n",
      "Unsupported spans:\n",
      "  - 'Profit margins improved significantly.'\n",
      "\n",
      "Per-span details:\n",
      "  [supported  ] conf=1.00 'Revenue grew 15% in Q4 2024.'...\n",
      "  [supported  ] conf=1.00 'The company plans to expand into 5 new markets.'...\n",
      "  [unsupported] conf=0.00 'Profit margins improved significantly.'...\n",
      "  [partial    ] conf=0.33 'The CEO will retire next year.'...\n"
     ]
    }
   ],
   "source": [
    "from cite_right import HallucinationConfig, check_groundedness\n",
    "\n",
    "answer = (\n",
    "    \"Revenue grew 15% in Q4 2024. \"\n",
    "    \"The company plans to expand into 5 new markets. \"\n",
    "    \"Profit margins improved significantly. \"\n",
    "    \"The CEO will retire next year.\"\n",
    ")\n",
    "\n",
    "sources = [\n",
    "    \"Annual report: Revenue grew 15% in Q4 2024, driven by strong holiday sales.\",\n",
    "    \"Press release: The company announced plans to expand into 5 new markets next year.\",\n",
    "]\n",
    "\n",
    "# Custom hallucination config\n",
    "hallucination_config = HallucinationConfig(\n",
    "    weak_citation_threshold=0.4,  # Below this is \"weak\" evidence\n",
    "    include_partial_in_grounded=True,  # Count partial matches in groundedness\n",
    ")\n",
    "\n",
    "metrics = check_groundedness(answer, sources, hallucination_config=hallucination_config)\n",
    "\n",
    "print(\"=== Groundedness Metrics ===\")\n",
    "print(f\"Groundedness Score: {metrics.groundedness_score:.1%}\")\n",
    "print(f\"Hallucination Rate: {metrics.hallucination_rate:.1%}\")\n",
    "print(f\"Average Confidence: {metrics.avg_confidence:.1%}\")\n",
    "print(f\"Minimum Confidence: {metrics.min_confidence:.1%}\")\n",
    "\n",
    "print(\"\\nSpan breakdown:\")\n",
    "print(f\"  Total spans: {metrics.num_spans}\")\n",
    "print(f\"  Supported: {metrics.num_supported} ({metrics.supported_ratio:.1%})\")\n",
    "print(f\"  Partial: {metrics.num_partial} ({metrics.partial_ratio:.1%})\")\n",
    "print(f\"  Unsupported: {metrics.num_unsupported} ({metrics.unsupported_ratio:.1%})\")\n",
    "print(f\"  Weak citations: {metrics.num_weak_citations}\")\n",
    "\n",
    "if metrics.unsupported_spans:\n",
    "    print(\"\\nUnsupported spans:\")\n",
    "    for span in metrics.unsupported_spans:\n",
    "        print(f\"  - {span.text!r}\")\n",
    "\n",
    "print(\"\\nPer-span details:\")\n",
    "for sc in metrics.span_confidences:\n",
    "    print(f\"  [{sc.status:11s}] conf={sc.confidence:.2f} {sc.span.text[:50]!r}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Annotating Answers with Citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown format (default):\n",
      "Revenue grew 15% in Q4.[1] The company will expand into 5 new markets.[2] Stock prices are expected to rise.[?]\n",
      "\n",
      "Superscript format:\n",
      "Revenue grew 15% in Q4.^1 The company will expand into 5 new markets.^2 Stock prices are expected to rise.[?]\n",
      "\n",
      "Footnote format:\n",
      "Revenue grew 15% in Q4.[^1] The company will expand into 5 new markets.[^2] Stock prices are expected to rise.[?]\n",
      "\n",
      "Without unsupported markers:\n",
      "Revenue grew 15% in Q4.[1] The company will expand into 5 new markets.[2] Stock prices are expected to rise.\n"
     ]
    }
   ],
   "source": [
    "from cite_right import SourceDocument, annotate_answer\n",
    "\n",
    "sources = [\n",
    "    SourceDocument(id=\"report\", text=\"Revenue grew 15% in Q4 2024.\"),\n",
    "    SourceDocument(id=\"press\", text=\"The company will expand into 5 new markets.\"),\n",
    "]\n",
    "\n",
    "answer = (\n",
    "    \"Revenue grew 15% in Q4. \"\n",
    "    \"The company will expand into 5 new markets. \"\n",
    "    \"Stock prices are expected to rise.\"\n",
    ")\n",
    "\n",
    "# Different annotation formats\n",
    "print(\"Markdown format (default):\")\n",
    "print(annotate_answer(answer, sources, format=\"markdown\"))\n",
    "\n",
    "print(\"\\nSuperscript format:\")\n",
    "print(annotate_answer(answer, sources, format=\"superscript\"))\n",
    "\n",
    "print(\"\\nFootnote format:\")\n",
    "print(annotate_answer(answer, sources, format=\"footnote\"))\n",
    "\n",
    "print(\"\\nWithout unsupported markers:\")\n",
    "print(annotate_answer(answer, sources, format=\"markdown\", include_unsupported=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Citation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citation Summary:\n",
      "- 0 of 3 spans fully supported\n",
      "- 2 spans partially supported\n",
      "- 1 spans unsupported\n",
      "- Sources cited: press_release, report_q4\n"
     ]
    }
   ],
   "source": [
    "from cite_right import SourceDocument, align_citations, get_citation_summary\n",
    "\n",
    "sources = [\n",
    "    SourceDocument(\n",
    "        id=\"report_q4\", text=\"Q4 revenue reached $50M, up 15% from last year.\"\n",
    "    ),\n",
    "    SourceDocument(\n",
    "        id=\"press_release\", text=\"The company announced 3 new product lines.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "answer = (\n",
    "    \"Revenue in Q4 was $50M, a 15% increase. \"\n",
    "    \"Three new products were announced. \"\n",
    "    \"The CEO expressed optimism about the future.\"\n",
    ")\n",
    "\n",
    "results = align_citations(answer, sources)\n",
    "summary = get_citation_summary(results)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Fact Verification\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Basic Fact Verification with `verify_facts()`\n",
    "\n",
    "Decompose answers into atomic claims and verify each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fact Verification Results ===\n",
      "Total claims: 3\n",
      "Verified: 2\n",
      "Partial: 0\n",
      "Unverified: 1\n",
      "Verification rate: 66.7%\n",
      "Average confidence: 75.0%\n",
      "\n",
      "Per-claim details:\n",
      "  [verified  ] conf=1.00 \"Revenue grew 15% in Q4.\"\n",
      "              sources: ['report']\n",
      "  [verified  ] conf=1.00 \"Profits increased by 20%.\"\n",
      "              sources: ['report']\n",
      "  [unverified] conf=0.25 \"The company plans to enter the Asian market.\"\n",
      "              sources: ['press']\n"
     ]
    }
   ],
   "source": [
    "from cite_right import FactVerificationConfig, SourceDocument, verify_facts\n",
    "\n",
    "sources = [\n",
    "    SourceDocument(\n",
    "        id=\"report\", text=\"Revenue grew 15% in Q4 2024. Profits increased by 20%.\"\n",
    "    ),\n",
    "    SourceDocument(id=\"press\", text=\"The company will expand into Europe next year.\"),\n",
    "]\n",
    "\n",
    "answer = (\n",
    "    \"Revenue grew 15% in Q4. \"\n",
    "    \"Profits increased by 20%. \"\n",
    "    \"The company plans to enter the Asian market.\"\n",
    ")\n",
    "\n",
    "# Configure verification thresholds\n",
    "config = FactVerificationConfig(\n",
    "    verified_coverage_threshold=0.6,  # Coverage needed for \"verified\" status\n",
    "    partial_coverage_threshold=0.3,  # Coverage needed for \"partial\" status\n",
    ")\n",
    "\n",
    "metrics = verify_facts(answer, sources, config=config)\n",
    "\n",
    "print(\"=== Fact Verification Results ===\")\n",
    "print(f\"Total claims: {metrics.num_claims}\")\n",
    "print(f\"Verified: {metrics.num_verified}\")\n",
    "print(f\"Partial: {metrics.num_partial}\")\n",
    "print(f\"Unverified: {metrics.num_unverified}\")\n",
    "print(f\"Verification rate: {metrics.verification_rate:.1%}\")\n",
    "print(f\"Average confidence: {metrics.avg_confidence:.1%}\")\n",
    "\n",
    "print(\"\\nPer-claim details:\")\n",
    "for v in metrics.claim_verifications:\n",
    "    print(f'  [{v.status:10s}] conf={v.confidence:.2f} \"{v.claim.text}\"')\n",
    "    if v.source_ids:\n",
    "        print(f\"              sources: {v.source_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Claim Decomposition\n",
    "\n",
    "Break sentences into atomic claims for finer-grained verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleClaimDecomposer: 1 claim(s)\n",
      "  [0:45] 'Revenue grew 15% and profits increased by 20%'\n",
      "\n",
      "SpacyClaimDecomposer: 2 claim(s)\n",
      "  [0:16] 'Revenue grew 15%'\n",
      "  [21:45] 'profits increased by 20%'\n"
     ]
    }
   ],
   "source": [
    "from cite_right import SimpleClaimDecomposer\n",
    "from cite_right.core.results import AnswerSpan\n",
    "\n",
    "# Simple decomposer (treats each span as one claim)\n",
    "simple_decomposer = SimpleClaimDecomposer()\n",
    "span = AnswerSpan(\n",
    "    text=\"Revenue grew 15% and profits increased by 20%\",\n",
    "    char_start=0,\n",
    "    char_end=45,\n",
    ")\n",
    "\n",
    "claims = simple_decomposer.decompose(span)\n",
    "print(f\"SimpleClaimDecomposer: {len(claims)} claim(s)\")\n",
    "for c in claims:\n",
    "    print(f\"  [{c.char_start}:{c.char_end}] {c.text!r}\")\n",
    "\n",
    "# spaCy-based decomposer (splits on conjunctions)\n",
    "try:\n",
    "    from cite_right import SpacyClaimDecomposer\n",
    "\n",
    "    spacy_decomposer = SpacyClaimDecomposer(\n",
    "        model=\"en_core_web_sm\",\n",
    "        min_claim_tokens=2,  # Minimum tokens for a valid claim\n",
    "    )\n",
    "\n",
    "    claims = spacy_decomposer.decompose(span)\n",
    "    print(f\"\\nSpacyClaimDecomposer: {len(claims)} claim(s)\")\n",
    "    for c in claims:\n",
    "        print(f\"  [{c.char_start}:{c.char_end}] {c.text!r}\")\n",
    "\n",
    "except RuntimeError as e:\n",
    "    print(f\"\\nSpacyClaimDecomposer not available: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 8: Framework Integrations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 LangChain Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain available: True\n",
      "Converted 2 documents\n",
      "Status: partial\n",
      "Chunk source_id: report.pdf\n",
      "Chunk doc_char_start: 100\n"
     ]
    }
   ],
   "source": [
    "from cite_right import (\n",
    "    align_citations,\n",
    "    from_langchain_chunks,\n",
    "    from_langchain_documents,\n",
    "    is_langchain_available,\n",
    ")\n",
    "\n",
    "print(f\"LangChain available: {is_langchain_available()}\")\n",
    "\n",
    "if is_langchain_available():\n",
    "    from langchain_core.documents import Document\n",
    "\n",
    "    # Simulate LangChain documents from a retriever\n",
    "    lc_docs = [\n",
    "        Document(\n",
    "            page_content=\"Revenue grew 15% in Q4.\", metadata={\"source\": \"report.pdf\"}\n",
    "        ),\n",
    "        Document(\n",
    "            page_content=\"The company expanded globally.\",\n",
    "            metadata={\"source\": \"news.txt\"},\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # Convert to cite-right format\n",
    "    sources = from_langchain_documents(lc_docs, id_key=\"source\")\n",
    "    print(f\"Converted {len(sources)} documents\")\n",
    "\n",
    "    # Use with align_citations\n",
    "    results = align_citations(\"Revenue increased by 15%.\", sources)\n",
    "    print(f\"Status: {results[0].status}\")\n",
    "\n",
    "    # For chunked documents with offsets\n",
    "    lc_chunks = [\n",
    "        Document(\n",
    "            page_content=\"Revenue grew 15%.\",\n",
    "            metadata={\"source\": \"report.pdf\", \"start_index\": 100, \"end_index\": 117},\n",
    "        ),\n",
    "    ]\n",
    "    chunk_sources = from_langchain_chunks(\n",
    "        lc_chunks,\n",
    "        id_key=\"source\",\n",
    "        start_key=\"start_index\",\n",
    "        end_key=\"end_index\",\n",
    "    )\n",
    "    print(f\"Chunk source_id: {chunk_sources[0].source_id}\")\n",
    "    print(f\"Chunk doc_char_start: {chunk_sources[0].doc_char_start}\")\n",
    "else:\n",
    "    print(\"Install with: pip install cite-right[langchain]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 LlamaIndex Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaIndex available: True\n",
      "Converted 2 nodes\n",
      "Status: partial\n",
      "Chunk source_id: report.pdf\n"
     ]
    }
   ],
   "source": [
    "from cite_right import (\n",
    "    align_citations,\n",
    "    from_llamaindex_chunks,\n",
    "    from_llamaindex_nodes,\n",
    "    is_llamaindex_available,\n",
    ")\n",
    "\n",
    "print(f\"LlamaIndex available: {is_llamaindex_available()}\")\n",
    "\n",
    "if is_llamaindex_available():\n",
    "    from llama_index.core.schema import TextNode\n",
    "\n",
    "    # Simulate LlamaIndex nodes from a retriever\n",
    "    nodes = [\n",
    "        TextNode(text=\"Revenue grew 15% in Q4.\", metadata={\"file_name\": \"report.pdf\"}),\n",
    "        TextNode(\n",
    "            text=\"The company expanded globally.\", metadata={\"file_name\": \"news.txt\"}\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # Convert to cite-right format\n",
    "    sources = from_llamaindex_nodes(nodes, id_key=\"file_name\")\n",
    "    print(f\"Converted {len(sources)} nodes\")\n",
    "\n",
    "    # Use with align_citations\n",
    "    results = align_citations(\"Revenue increased by 15%.\", sources)\n",
    "    print(f\"Status: {results[0].status}\")\n",
    "\n",
    "    # For nodes with character offsets\n",
    "    chunked_nodes = [\n",
    "        TextNode(\n",
    "            text=\"Revenue grew 15%.\",\n",
    "            metadata={\n",
    "                \"file_name\": \"report.pdf\",\n",
    "                \"start_char_idx\": 100,\n",
    "                \"end_char_idx\": 117,\n",
    "            },\n",
    "        ),\n",
    "    ]\n",
    "    chunk_sources = from_llamaindex_chunks(\n",
    "        chunked_nodes,\n",
    "        id_key=\"file_name\",\n",
    "        start_key=\"start_char_idx\",\n",
    "        end_key=\"end_char_idx\",\n",
    "    )\n",
    "    print(f\"Chunk source_id: {chunk_sources[0].source_id}\")\n",
    "else:\n",
    "    print(\"Install with: pip install cite-right[llamaindex]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 9: Understanding Citation Scores\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Score Components Explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'Revenue increased by 15.3% in Q4 2024.'\n",
      "\n",
      "Citation from 'exact':\n",
      "  Final score: 2.438\n",
      "  Evidence: 'Revenue increased by exactly 15.3% in Q4 2024'\n",
      "  \n",
      "Score components:\n",
      "    alignment_score: 15 (raw Smith-Waterman score)\n",
      "    normalized_alignment: 0.938 (alignment / max possible)\n",
      "    matches: 8 (number of matched tokens)\n",
      "    answer_coverage: 100.0% (fraction of answer matched)\n",
      "    evidence_coverage: 88.9% (fraction of evidence matched)\n",
      "    lexical_score: 1.000 (IDF-weighted overlap)\n",
      "    embedding_score: 0.000 (cosine similarity)\n",
      "    embedding_only: 0 (1 if no lexical match)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from cite_right import SourceDocument, align_citations\n",
    "\n",
    "sources = [\n",
    "    SourceDocument(id=\"exact\", text=\"Revenue increased by exactly 15.3% in Q4 2024.\"),\n",
    "    SourceDocument(\n",
    "        id=\"partial\", text=\"Company financials showed growth in the fourth quarter.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "answer = \"Revenue increased by 15.3% in Q4 2024.\"\n",
    "results = align_citations(answer, sources)\n",
    "\n",
    "print(f\"Answer: {results[0].answer_span.text!r}\\n\")\n",
    "\n",
    "for citation in results[0].citations:\n",
    "    print(f\"Citation from '{citation.source_id}':\")\n",
    "    print(f\"  Final score: {citation.score:.3f}\")\n",
    "    print(f\"  Evidence: {citation.evidence!r}\")\n",
    "    print(\"  \\nScore components:\")\n",
    "\n",
    "    c = citation.components\n",
    "    print(\n",
    "        f\"    alignment_score: {c.get('alignment_score', 0):.0f} (raw Smith-Waterman score)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"    normalized_alignment: {c.get('normalized_alignment', 0):.3f} (alignment / max possible)\"\n",
    "    )\n",
    "    print(f\"    matches: {c.get('matches', 0):.0f} (number of matched tokens)\")\n",
    "    print(\n",
    "        f\"    answer_coverage: {c.get('answer_coverage', 0):.1%} (fraction of answer matched)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"    evidence_coverage: {c.get('evidence_coverage', 0):.1%} (fraction of evidence matched)\"\n",
    "    )\n",
    "    print(f\"    lexical_score: {c.get('lexical_score', 0):.3f} (IDF-weighted overlap)\")\n",
    "    print(f\"    embedding_score: {c.get('embedding_score', 0):.3f} (cosine similarity)\")\n",
    "    print(\n",
    "        f\"    embedding_only: {c.get('embedding_only', 0):.0f} (1 if no lexical match)\"\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Customizing Score Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default    weights: final_score=1.688\n",
      "lexical    weights: final_score=2.156\n",
      "coverage   weights: final_score=2.823\n"
     ]
    }
   ],
   "source": [
    "from cite_right import CitationConfig, CitationWeights, SourceDocument, align_citations\n",
    "\n",
    "sources = [\n",
    "    SourceDocument(id=\"doc\", text=\"The quick brown fox jumps over the lazy dog.\")\n",
    "]\n",
    "answer = \"The brown fox jumped over a lazy dog.\"\n",
    "\n",
    "# Default weights\n",
    "default_weights = (\n",
    "    CitationWeights()\n",
    ")  # alignment=1.0, answer_coverage=1.0, lexical=0.5, embedding=0.5\n",
    "\n",
    "# Emphasize lexical matching\n",
    "lexical_weights = CitationWeights(\n",
    "    alignment=0.5,\n",
    "    answer_coverage=0.5,\n",
    "    evidence_coverage=0.0,\n",
    "    lexical=2.0,  # Double weight for lexical\n",
    "    embedding=0.0,\n",
    ")\n",
    "\n",
    "# Emphasize coverage\n",
    "coverage_weights = CitationWeights(\n",
    "    alignment=0.5,\n",
    "    answer_coverage=2.0,  # Double weight for coverage\n",
    "    evidence_coverage=1.0,\n",
    "    lexical=0.5,\n",
    "    embedding=0.5,\n",
    ")\n",
    "\n",
    "for name, weights in [\n",
    "    (\"default\", default_weights),\n",
    "    (\"lexical\", lexical_weights),\n",
    "    (\"coverage\", coverage_weights),\n",
    "]:\n",
    "    config = CitationConfig(weights=weights)\n",
    "    results = align_citations(answer, sources, config=config)\n",
    "    if results[0].citations:\n",
    "        score = results[0].citations[0].score\n",
    "        print(f\"{name:10s} weights: final_score={score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 10: Full Example - RAG Post-Processing Pipeline\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RAG POST-PROCESSING PIPELINE\n",
      "============================================================\n",
      "\n",
      "📊 Step 1: Groundedness Check\n",
      "   Groundedness: 31.1%\n",
      "   Hallucination rate: 68.9%\n",
      "   Supported/Partial/Unsupported: 1/2/1\n",
      "\n",
      "⚠️  Step 2: Problematic Spans\n",
      "   UNSUPPORTED: 'The authors expect it to revolutionize the field.'\n",
      "   WEAK: 'It achieves 19% better performance than GRPO while using 35x fewer samples.'\n",
      "   WEAK: 'Compared to MIPROv2, GEPA achieves +14% optimization gains.'\n",
      "\n",
      "📝 Step 3: Annotated Answer\n",
      "   GEPA is a new prompt optimization method that combines reflection with evolutionary search.[1] It achieves 19% better performance than GRPO while using 35x fewer samples.[2] Compared to MIPROv2, GEPA achieves +14% optimization gains.[3] The authors expect it to revolutionize the field.[?]\n",
      "\n",
      "📋 Step 4: Citation Details\n",
      "\n",
      "   Sentence 1: GEPA is a new prompt optimization method that combines refle...\n",
      "   Status: supported\n",
      "   Best source: gepa_intro\n",
      "   Coverage: 61.5%\n",
      "\n",
      "   Sentence 2: It achieves 19% better performance than GRPO while using 35x...\n",
      "   Status: partial\n",
      "   Best source: grpo_comparison\n",
      "   Coverage: 21.4%\n",
      "\n",
      "   Sentence 3: Compared to MIPROv2, GEPA achieves +14% optimization gains....\n",
      "   Status: partial\n",
      "   Best source: mipro_comparison\n",
      "   Coverage: 22.2%\n",
      "\n",
      "   Sentence 4: The authors expect it to revolutionize the field....\n",
      "   Status: unsupported\n"
     ]
    }
   ],
   "source": [
    "from cite_right import (\n",
    "    CitationConfig,\n",
    "    SourceDocument,\n",
    "    align_citations,\n",
    "    annotate_answer,\n",
    "    check_groundedness,\n",
    ")\n",
    "\n",
    "# Simulate a RAG pipeline output\n",
    "retrieved_sources = [\n",
    "    SourceDocument(\n",
    "        id=\"gepa_intro\",\n",
    "        text=\"We introduce GEPA (Genetic-Pareto), a reflective prompt optimizer \"\n",
    "        \"that merges textual reflection with multi-objective evolutionary search.\",\n",
    "    ),\n",
    "    SourceDocument(\n",
    "        id=\"grpo_comparison\",\n",
    "        text=\"On Qwen3 8B, GEPA outperforms GRPO by up to 19% while requiring \"\n",
    "        \"up to 35x fewer rollouts.\",\n",
    "    ),\n",
    "    SourceDocument(\n",
    "        id=\"mipro_comparison\",\n",
    "        text=\"GEPA surpasses MIPROv2 on every benchmark, obtaining aggregate \"\n",
    "        \"optimization gains of +14%, more than doubling MIPROv2's +7%.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "llm_answer = (\n",
    "    \"GEPA is a new prompt optimization method that combines reflection with evolutionary search. \"\n",
    "    \"It achieves 19% better performance than GRPO while using 35x fewer samples. \"\n",
    "    \"Compared to MIPROv2, GEPA achieves +14% optimization gains. \"\n",
    "    \"The authors expect it to revolutionize the field.\"\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RAG POST-PROCESSING PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Check overall groundedness\n",
    "print(\"\\n📊 Step 1: Groundedness Check\")\n",
    "metrics = check_groundedness(llm_answer, retrieved_sources)\n",
    "print(f\"   Groundedness: {metrics.groundedness_score:.1%}\")\n",
    "print(f\"   Hallucination rate: {metrics.hallucination_rate:.1%}\")\n",
    "print(\n",
    "    f\"   Supported/Partial/Unsupported: {metrics.num_supported}/{metrics.num_partial}/{metrics.num_unsupported}\"\n",
    ")\n",
    "\n",
    "# Step 2: Identify problematic spans\n",
    "print(\"\\n⚠️  Step 2: Problematic Spans\")\n",
    "if metrics.unsupported_spans:\n",
    "    for span in metrics.unsupported_spans:\n",
    "        print(f\"   UNSUPPORTED: {span.text!r}\")\n",
    "if metrics.weakly_supported_spans:\n",
    "    for span in metrics.weakly_supported_spans:\n",
    "        print(f\"   WEAK: {span.text!r}\")\n",
    "\n",
    "# Step 3: Generate annotated answer\n",
    "print(\"\\n📝 Step 3: Annotated Answer\")\n",
    "annotated = annotate_answer(llm_answer, retrieved_sources)\n",
    "print(f\"   {annotated}\")\n",
    "\n",
    "# Step 4: Detailed citation breakdown\n",
    "print(\"\\n📋 Step 4: Citation Details\")\n",
    "results = align_citations(llm_answer, retrieved_sources)\n",
    "for i, sc in enumerate(results):\n",
    "    print(f\"\\n   Sentence {i + 1}: {sc.answer_span.text[:60]}...\")\n",
    "    print(f\"   Status: {sc.status}\")\n",
    "    if sc.citations:\n",
    "        best = sc.citations[0]\n",
    "        print(f\"   Best source: {best.source_id}\")\n",
    "        print(f\"   Coverage: {best.components.get('answer_coverage', 0):.1%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cite-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
